{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35aebf2e-0635-4fef-bc9a-877b6a20fb13",
   "metadata": {},
   "source": [
    "![Credit card being held in hand](credit_card.jpg)\n",
    "\n",
    "Commercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n",
    "\n",
    "### The Data\n",
    "\n",
    "The data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e86b1e8-a3fa-4b09-982f-795f218bd1a6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 35,
    "lastExecutedAt": 1732246927860,
    "lastExecutedByKernel": "f54c497a-2fa7-46da-8586-e5408677a749",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()\n",
    "outputsMetadata": {
     "0": {
      "height": 222,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>g</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>g</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>g</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>s</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0      1      2  3  4  5  6     7  8  9   10 11   12 13\n",
       "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  g    0  +\n",
       "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  g  560  +\n",
       "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  g  824  +\n",
       "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  g    3  +\n",
       "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  s    0  +"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None)  # Load the credit card approvals dataset\n",
    "cc_apps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a47990-e0c7-4c13-b8f4-2af2e8efb644",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1732246927965,
    "lastExecutedByKernel": "f54c497a-2fa7-46da-8586-e5408677a749",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "letter_idx = [0,3,4,5,6,8,9,11,13]\nnum_idx = [1,2,7,10,12]\n\nfor idx in letter_idx:\n    max_count_value = cc_apps[idx].value_counts().idxmax()\n    cc_apps[idx] = cc_apps[idx].replace('?', max_count_value)\n\nfor idx in num_idx:\n    cc_apps[idx] = pd.to_numeric(cc_apps[idx], errors='coerce')\n    average_value = cc_apps[idx].mean()\n    cc_apps[idx] = cc_apps[idx].fillna(average_value)\n\nfor idx in range(14):\n    cc_apps[idx] = LabelEncoder().fit_transform(cc_apps[idx])\n\n",
    "outputsMetadata": {
     "0": {
      "height": 508,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Columns with categorical (non-numeric) data\n",
    "letter_idx = [0, 3, 4, 5, 6, 8, 9, 11, 13]\n",
    "# Columns with numeric data\n",
    "num_idx = [1, 2, 7, 10, 12]\n",
    "\n",
    "# Replace '?' in categorical columns with the most frequent value (mode)\n",
    "for idx in letter_idx:\n",
    "    max_count_value = cc_apps[idx].value_counts().idxmax()\n",
    "    cc_apps[idx] = cc_apps[idx].replace('?', max_count_value)\n",
    "\n",
    "# Replace '?' in numeric columns with the mean of the column\n",
    "for idx in num_idx:\n",
    "    cc_apps[idx] = pd.to_numeric(cc_apps[idx], errors='coerce')  # Convert to numeric\n",
    "    average_value = cc_apps[idx].mean()\n",
    "    cc_apps[idx] = cc_apps[idx].fillna(average_value)  # Replace NaN with mean\n",
    "\n",
    "# Encode all columns to numeric values using LabelEncoder\n",
    "for idx in range(14):\n",
    "    cc_apps[idx] = LabelEncoder().fit_transform(cc_apps[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abfe2265-ab9a-4de8-a48c-2d244d94a5bd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1732246928017,
    "lastExecutedByKernel": "f54c497a-2fa7-46da-8586-e5408677a749",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "X = cc_apps.drop(13,axis = 1)\nY = cc_apps[[13]]\n\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,train_size = 0.8,random_state = 42)\n\nscaler_train = StandardScaler()\nX_train_scale = scaler_train.fit_transform(X_train)\nX_train_scale = torch.tensor(X_train_scale, dtype=torch.float)\nY_train = torch.tensor(Y_train.values, dtype=torch.float)\n\n\nscaler_test = StandardScaler()\nX_test_scale = scaler_test.fit_transform(X_test)\nX_test_scale = torch.tensor(X_test_scale, dtype=torch.float)\nY_test = torch.tensor(Y_test.values, dtype=torch.float)\n\n",
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (Y)\n",
    "X = cc_apps.drop(13, axis=1)  # Features\n",
    "Y = cc_apps[[13]]  # Target variable\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Standardize the training features\n",
    "scaler_train = StandardScaler()\n",
    "X_train_scale = scaler_train.fit_transform(X_train)  # Fit and transform training data\n",
    "X_train_scale = torch.tensor(X_train_scale, dtype=torch.float)  # Convert to PyTorch tensor\n",
    "Y_train = torch.tensor(Y_train.values, dtype=torch.float)  # Convert target to PyTorch tensor\n",
    "\n",
    "# Standardize the testing features\n",
    "scaler_test = StandardScaler()\n",
    "X_test_scale = scaler_test.fit_transform(X_test)  # Fit and transform testing data\n",
    "X_test_scale = torch.tensor(X_test_scale, dtype=torch.float)  # Convert to PyTorch tensor\n",
    "Y_test = torch.tensor(Y_test.values, dtype=torch.float)  # Convert target to PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98aab626-0f68-4d1b-8b9b-153dd479986b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1732246928069,
    "lastExecutedByKernel": "f54c497a-2fa7-46da-8586-e5408677a749",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "class MultiLayerNN(nn.Module):\n    def __init__(self,in_dim,hidden_dim,out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim,hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n        self.output = nn.Linear(hidden_dim,out_dim)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self,x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc2(x))\n        x = self.sigmoid(self.output(x))\n        return x\n    \nepochs = 600\nlearning_rate = 0.15\nmodel = MultiLayerNN(X_train_scale.shape[1],256,1)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\nmodel.train()\n\ndataset_train = TensorDataset(X_train_scale,Y_train)\ndataloader_train = DataLoader(dataset_train, batch_size = 10,shuffle = True)\n\ndataset_test = TensorDataset(X_test_scale,Y_test)\ndataloader_test = DataLoader(dataset_test, batch_size = 10,shuffle = True)\n"
   },
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class MultiLayerNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # Second fully connected layer\n",
    "        self.output = nn.Linear(hidden_dim, out_dim)  # Output layer\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))  # Adding multiple passes through the hidden layer\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.output(x))  # Final sigmoid activation for output\n",
    "        return x\n",
    "\n",
    "# Initialize hyperparameters and the model\n",
    "epochs = 600  # Number of training epochs\n",
    "learning_rate = 0.15  # Learning rate for optimizer\n",
    "model = MultiLayerNN(X_train_scale.shape[1], 256, 1)  # Create the model\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # SGD optimizer\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "# Create DataLoader for training and testing datasets\n",
    "dataset_train = TensorDataset(X_train_scale, Y_train)  # Training dataset\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=10, shuffle=True)  # DataLoader for training\n",
    "\n",
    "dataset_test = TensorDataset(X_test_scale, Y_test)  # Testing dataset\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)  # DataLoader for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc18a80e-131e-4426-adce-0976877844aa",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 269,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss of Epoch 50: 0.019190441194485937\n",
      "Training loss of Epoch 100: 0.005411607967778208\n",
      "Training loss of Epoch 150: 0.005028096604128507\n",
      "Training loss of Epoch 200: 0.004890313664781534\n",
      "Training loss of Epoch 250: 0.010260682418304922\n",
      "Training loss of Epoch 300: 0.00435260547092498\n",
      "Training loss of Epoch 350: 0.00448014042639456\n",
      "Training loss of Epoch 400: 0.003321779052191922\n",
      "Training loss of Epoch 450: 0.002221178280446594\n",
      "Training loss of Epoch 500: 0.002076605235285636\n",
      "Training loss of Epoch 550: 0.004845103858863731\n",
      "Training loss of Epoch 600: 0.005474136827418812\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    training_losses = []  # To track training loss for each epoch\n",
    "    for batch in dataloader_train:\n",
    "        x, y = batch[0], batch[1]  # Get a batch of data\n",
    "        optimizer.zero_grad()  # Zero the gradient buffers\n",
    "        pred = model(x)  # Forward pass\n",
    "        loss = criterion(pred, y)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "        training_losses.append(loss.item())  # Store the loss\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Training loss of Epoch {epoch + 1}: {np.mean(training_losses)}\")  # Print loss every 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa919e7-1a32-4141-a513-5a032368ec99",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1732246963461,
    "lastExecutedByKernel": "f54c497a-2fa7-46da-8586-e5408677a749",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "with torch.no_grad():\n    model.eval()\n    num_correct = 0\n    total_size = 0\n    for batch in dataloader_train:\n        x,y = batch[0],batch[1]\n        pred = model(x)   \n        pred_prob = torch.sigmoid(pred)\n        pred_labels = (pred_prob > 0.5).float()\n        num_correct += (pred_labels == y).sum().item()\n        total_size += len(x)\n    best_score = num_correct/total_size\n    print(f\"Accuracy for training data: {best_score}\")",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training data: 0.9057971014492754\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training data\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    num_correct = 0\n",
    "    total_size = 0\n",
    "    for batch in dataloader_train:\n",
    "        x, y = batch[0], batch[1]\n",
    "        pred = model(x)  # Forward pass\n",
    "        pred_prob = torch.sigmoid(pred)  # Apply sigmoid to get probabilities\n",
    "        pred_labels = (pred_prob > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        num_correct += (pred_labels == y).sum().item()  # Count correct predictions\n",
    "        total_size += len(x)  # Count total samples\n",
    "    best_score = num_correct / total_size  # Calculate accuracy\n",
    "    print(f\"Accuracy for training data: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5d4f9-7fd7-46f5-90c6-f8ed7b666e56",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1732246963514,
    "lastExecutedByKernel": "f54c497a-2fa7-46da-8586-e5408677a749",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "with torch.no_grad():\n    model.eval()\n    num_correct = 0\n    total_size = 0\n    for batch in dataloader_test:\n        x,y = batch[0],batch[1]\n        pred = model(x)   \n        pred_prob = torch.sigmoid(pred)\n        pred_labels = (pred_prob > 0.5).float()\n        num_correct += (pred_labels == y).sum().item()\n        total_size += len(x)\n    print(f\"Accuracy for testing data: {num_correct/total_size}\")",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for testing data: 0.7391304347826086\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    num_correct = 0\n",
    "    total_size = 0\n",
    "    for batch in dataloader_test:\n",
    "        x, y = batch[0], batch[1]\n",
    "        pred = model(x)  # Forward pass\n",
    "        pred_prob = torch.sigmoid(pred)  # Apply sigmoid to get probabilities\n",
    "        pred_labels = (pred_prob > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        num_correct += (pred_labels == y).sum().item()  # Count correct predictions\n",
    "        total_size += len(x)  # Count total samples\n",
    "    print(f\"Accuracy for testing data: {num_correct / total_size}\")  # Print testing accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
